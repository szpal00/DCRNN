{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBnTanh(nn.Module):\n",
    "    def __init__(self,input_size, output_size, seq_len = 12):\n",
    "        super(DBnTanh, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.output_size),\n",
    "            nn.BatchNorm1d(self.seq_len),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_preprocess(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size = 256, device=0):\n",
    "        super(CNN_preprocess, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = nn.Sequential(nn.Flatten(),\n",
    "                                    DBnTanh(self.input_size[0]*self.input_size[1],1000, seq_len=1000),\n",
    "                                    nn.BatchNorm1d(1000),\n",
    "                                    nn.Linear(1000, self.output_size)\n",
    "                                   )\n",
    "\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size = 256, device=0):\n",
    "        super(DenseModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = nn.Sequential(nn.Flatten(start_dim = 2),\n",
    "                                    DBnTanh(self.input_size[1]*self.input_size[2],1000, seq_len=12),\n",
    "                                    nn.BatchNorm1d(12),\n",
    "                                    nn.Linear(1000, self.output_size)\n",
    "                                   )\n",
    "\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size,  output_size,model, batch_size = 256, device=0):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.preprocess = model\n",
    "        self.freeze_weights()\n",
    "        \n",
    "        self.conv = nn.Sequential(nn.Conv1d(2*207, 512, 3, padding = 1, stride = 2), #512, 6\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv1d(512, 1024, 3, padding = 1, stride = 2), #1024,3\n",
    "                                  nn.BatchNorm1d(1024),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv1d(1024, 1024, 3, padding = 0, stride = 1), #1024,1\n",
    "                                  nn.BatchNorm1d(1024),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Flatten()\n",
    "        \n",
    "        )\n",
    "        self.dense = nn.Linear(1024, self.output_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.extract_features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extract_features(self,x):\n",
    "        y = torch.split(x,1,1)\n",
    "        ret = []\n",
    "        for tens in y:\n",
    "            tmp = self.preprocess(tens)\n",
    "            ret.append(torch.reshape(tmp,(tmp.shape[0],1,tmp.shape[1])))\n",
    "        ret = torch.cat(ret,1)\n",
    "        return torch.transpose(ret, 1,2)\n",
    "    \n",
    "    def freeze_weights(self):\n",
    "        for param in self.preprocess.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def load_weights(self, model):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,input_size, output_size, num_directions = 1, mapping_dim = 1000, batch_size=32, device = 0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = 12\n",
    "        self.n_hidden = 50 # number of hidden states\n",
    "        self.n_layers = 10 # number of LSTM layers (stacked)\n",
    "        self.dir = num_directions\n",
    "        self.mapping_dim = mapping_dim\n",
    "        self.latent_dim = 200\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = self.input_size, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True,\n",
    "                                 bidirectional = bool(num_directions-1),\n",
    "                                 dropout = 0.5)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        self.dense_ = nn.Linear(self.input_size,self.mapping_dim)\n",
    "        self.dense1 = DBnTanh(self.n_hidden,1000)\n",
    "        self.dense2 = nn.Linear(1000,self.output_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(self.seq_len)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        # hidden_state = torch.zeros(self.dir*self.n_layers,batch_size,self.n_hidden).to(device)\n",
    "        # cell_state = torch.zeros(self.dir*self.n_layers,batch_size,self.n_hidden).to(device)\n",
    "        hidden_state = torch.randn(self.dir*self.n_layers,batch_size,self.n_hidden).to(device)\n",
    "        cell_state = torch.randn(self.dir*self.n_layers,batch_size,self.n_hidden).to(device)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        lstm_out, __ = self.l_lstm(x, self.hidden)\n",
    "        x = lstm_out\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "       \n",
    "        \n",
    "        \n",
    "       \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
